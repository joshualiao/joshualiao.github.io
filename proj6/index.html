<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title> CS180 Project 6 Image Gallery</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script>
        window.MathJax = {
          tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']], // For inline math
            displayMath: [['$$', '$$'], ['\\[', '\\]']], // For display math
            tags: 'all', // Enables equation numbering across the document
            packages: { '[+]': ['ams'] } // Adds support for AMS environments like `equation`
          }
        };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        body {
            font-family: "Times New Roman", Times, serif;
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        header {
            background-color: #f4f4f4;
            padding: 20px;
            text-align: center;
            border-bottom: 2px solid #4b4b4b;
        }
        header h1 {
            margin: 0;
            font-size: 2.5em;
            color: #000000;
        }
        header h2 {
            margin-top: 10px;
            font-size: 1.5em;
            color: #000000;
        }
        header p {
            margin-top: 15px;
            margin-bottom: 20px;
            font-size: 1em;
            color: #000000;
        }
        .gallery-container {
            margin: 40px;
            border: 10px solid #d4af37;
            border-radius: 15px;
            background: #f3e9dc;
            padding: 20px;
            box-shadow: 0 0 20px rgba(0, 0, 0, 0.5),
                        0 0 0 15px rgba(228, 228, 227, 0.6),
                        inset 0 0 10px rgba(255, 255, 255, 0.3);
        }
        /* Grid layout for seven images */
        .seven-images {
            display: grid;
            grid-template-columns: repeat(9, 1fr); /* 3 equal columns */
            gap: 10px;
            margin-bottom: 10px;
        }
        .seven-images .image-item:nth-child(1) {
            grid-column: 2 / 3
        }
        .five-small-images {
            display: grid;
            grid-template-columns: repeat(7, 1fr);
            gap: 10px;
            margin-bottom: 10px;
        }
        .five-small-images .image-item:nth-child(1) {
            grid-column: 2 / 3
        }
        /* Grid layout for four images */
        .four-images {
            display: grid;
            grid-template-columns: repeat(4, 1fr); /* 3 equal columns */
            gap: 10px;
            margin-bottom: 10px;
        }
        .four-small-images {
            display: grid;
            grid-template-columns: repeat(8, 1fr);
            gap: 10px;
            margin-bottom: 10px;
        }
        .four-small-images .image-item:nth-child(1) {
            grid-column: 3 / 4
        }
        /* Grid layout for three images */
        .three-images {
            display: grid;
            grid-template-columns: repeat(3, 1fr); /* 3 equal columns */
            gap: 10px;
            margin-bottom: 10px;
        }
        .three-small-images {
            display: grid;
            grid-template-columns: repeat(7, 1fr);
            gap: 10px;
            margin-bottom: 10px;
        }
        .three-small-images .image-item:nth-child(1) {
            grid-column: 3 / 4
        }
        /* Grid layout for two images */
        .two-images {
            display: grid;
            grid-template-columns: repeat(3, 1fr); /* 3 equal columns */
            gap: 10px;
        }
        /* Center the two images in the grid by leaving an empty column on both sides */
        .two-images .image-item:nth-child(1) {
            grid-column: 2 / 3; /* First image goes in the center column */
        }
        .two-images-center {
            display: grid;
            grid-template-columns: repeat(4, 1fr); /* 3 equal columns */
            gap: 10px;
        }
        .two-images-center .image-item:nth-child(1) {
            grid-column: 2 / 3; /* First image goes in the center column */
        }
        .one-image {
            display: grid;
            grid-template-columns: repeat(1, 1fr); /* 3 equal columns */
            gap: 10px;
        }
        /* Shared image item styling */
        .image-item img {
            width: 100%;
            height: auto;
            max-width: 100%;
            max-height: 100%;
            object-fit: cover;
            border-radius: 0; /* Removes rounded corners */
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1); /* Subtle shadow */
        }
        .gallery-header {
            grid-column: span 3; /* Header spans all columns */
            text-align: center;
            margin-bottom: 5px;
        }
        .gallery-header h3 {
            margin: 0;
            font-size: 1.2em;
            color: #000000;
        }
        .gallery-header p {
            margin: 100px;
            margin-top: 5px;
            margin-bottom: 20px;
            font-size: 1em;
            color: #000000;
        }
        .description {
            margin-top: 5px;
            font-size: 14px;
            text-align: center;
            color: #333;
            margin-bottom: 10px;
        }
        .discussion-section {
            background-color: #f9f9f9;
            padding-left: 250px;
            padding-right: 250px;
            margin-top: 10px;
        }
        .discussion-section h2 {
            font-size: 1.8em;
            margin-bottom: 10px;
            color: #000000;
        }
        .discussion-section p {
            font-size: 1em;
            color: #000000;
            line-height: 1.6;
        }
    </style>
</head>
<body>

    <!-- Header Section -->
    <header>
        <h1> Neural Radiance Fields </h1>
        <h2> CS180, Joshua Liao </h2>
        <p> 
            NeRF
        </p>
    </header>

    <!-- Image Gallery Section -->
    <div class="gallery-container">
        <div class="gallery-header">
            <h3> Overview </h3>
            <p>
            In this project, we implement/reproduce some results from the 
             <a href="https://www.matthewtancik.com/nerf">Neural Radiance Field</a> paper. 
            <br>
            We train a neural net to model a 3D scene from 2D data. The scene is represented as 5D-function, consisting of 3D points 
            and a 2D viewing direction. The neural net learns to model the scene by learning to predict what color is seen at
            some 3D point given some 2D viewing direction. The training data is a set of calibrated cameras, whose locations in comparision 
            to the object of interest (a lego truck in this case!) are known, as well as focal lengths. 
            </p>
        </div>
</div>

<div class="gallery-container">
    <div class="gallery-header">
        <h3> 2D Neural Fields </h3> <p>
        We first implement a simplified, 2D version, a neural field. This is training a neural net to encode an image: input a pixel's (x, y) location, and output 
        some RBG. (It works out to be a very inefficient way to compress data).
        <br>
        Our neural net uses positional encoding (feature augmentation of (x, y) -> (x, y, sin(pi*x), cos(pi*x), sin(2pi*x)...)), with L = 10. 
        Then we have four hidden fully connected layers with ReLU, using hidden size 256. 
        We use a sigmoid activation at the end to bound outputs to valid pixel values between 0 and 1.
        We train by sampling 10,000 different pixels from the image, using learning rate 0.01 and default Adam parameters. 
        We use PSNR loss. We train for 2500 iterations. 
        <br> 
        We also do some hyperparameter tuning, and show some end results for varying the hidden size of the model and the positional encoding frequency.
        </p>
    </div>

    <div class="two-images">
        <div class="image-item">
            <img src="images/model1.png" alt="Description of Image 1">
            <div class="description"> 
                Model architecture.
            </div>
        </div>
    </div>

    <div class="three-images">
        <div class="image-item">
            <img src="images/part1fox/fox.png" alt="Description of Image 1">
            <div class="description"> 
                An image of a fox.
            </div>
        </div>
        <div class="image-item">
            <img src="images/part1fox/base_epoch1.png" alt="Description of Image 1">
            <div class="description"> 
                Epoch 1 (500 iterations), with base architecture
            </div>
        </div>
        <div class="image-item">
            <img src="images/part1fox/base_epoch2.png" alt="Description of Image 1">
            <div class="description"> 
                Epoch 2 (1000 iterations)
            </div>
        </div>
    </div>

    <div class="three-images">
        <div class="image-item">
            <img src="images/part1fox/base_epoch3.png" alt="Description of Image 1">
            <div class="description"> 
                Epoch 3 (1500 iterations)
            </div>
        </div>
        <div class="image-item">
            <img src="images/part1fox/base_epoch4.png" alt="Description of Image 1">
            <div class="description"> 
                Epoch 4 (2000 iterations)
            </div>
        </div>
        <div class="image-item">
            <img src="images/part1fox/base_epoch5.png" alt="Description of Image 1">
            <div class="description"> 
                Epoch 5 (2500 iterations)
            </div>
        </div>
    </div>

    <div class="two-images">
        <div class="image-item">
            <img src="images/part1fox/base_losscurve.png" alt="Description of Image 1">
            <div class="description"> 
                Loss curve for base architecture above.
            </div>
        </div>
    </div>

    <div class="three-images">
        <div class="image-item">
            <img src="images/part1fox/h128_epoch5.png" alt="Description of Image 1">
            <div class="description"> 
                Using hidden size 128; epoch 5.
            </div>
        </div>
        <div class="image-item">
            <img src="images/part1fox/base_epoch5.png" alt="Description of Image 1">
            <div class="description"> 
                Using hidden size 256; epoch 5 (also seen above).
            </div>
        </div>
        <div class="image-item">
            <img src="images/part1fox/h400_epoch5.png" alt="Description of Image 1">
            <div class="description"> 
                Using hidden size 400; epoch 5. 
            </div>
        </div>
    </div>

    <div class="three-images">
        <div class="image-item">
            <img src="images/part1fox/h128_losscurve.png" alt="Description of Image 1">
            <div class="description"> 
                Loss curve for hidden size h=128
            </div>
        </div>
        <div class="image-item">
            <img src="images/part1fox/base_losscurve.png" alt="Description of Image 1">
            <div class="description"> 
                Loss curve for hidden size h=256. Slight improvement over 128.
            </div>
        </div>
        <div class="image-item">
            <img src="images/part1fox/h400_losscurve.png" alt="Description of Image 1">
            <div class="description"> 
                Loss curve for hidden size h=400. Has very little improvement over 256.
            </div>
        </div>
    </div>

    <div class="three-images">
        <div class="image-item">
            <img src="images/part1fox/L2_epoch5.png" alt="Description of Image 1">
            <div class="description"> 
                Using positional encoding frequency L=2. This result is pretty cool— without the higher frequency feature augmentation of the input, 
                the neural net learns a lower frequency representation of the image! 
            </div>
        </div>
        <div class="image-item">
            <img src="images/part1fox/base_epoch5.png" alt="Description of Image 1">
            <div class="description"> 
                Using L=10; epoch 5 (also seen above).
            </div>
        </div>
        <div class="image-item">
            <img src="images/part1fox/L20_epoch5.png" alt="Description of Image 1">
            <div class="description"> 
                Using L=20.
            </div>
        </div>
    </div>

    <div class="three-images">
        <div class="image-item">
            <img src="images/part1fox/L2_losscurve.png" alt="Description of Image 1">
            <div class="description"> 
                L=2 loss curve. Interestingly, this loss curve is much less stable than the other loss curves, 
                as well as not breaking past -60 PSNR loss.
            </div>
        </div>
        <div class="image-item">
            <img src="images/part1fox/base_losscurve.png" alt="Description of Image 1">
            <div class="description"> 
                Using L=10 loss curve
            </div>
        </div>
        <div class="image-item">
            <img src="images/part1fox/L20_losscurve.png" alt="Description of Image 1">
            <div class="description"> 
                Using L=20 loss curve
            </div>
        </div>
    </div>
</div>

<div class="gallery-container">
    <div class="gallery-header">
        <h3> 3D Neural Radiance Fields </h3>
        The Neural Radiance Field algorithm involves training a neural net to take in a 3D world coordinate and a viewing direction, 
        and then outputting a density and RGB. We can reconstruct the model's prediction of an image by asking it to predict all the rays consistent with a camera
        location. During training, we sample random rays from our training camera, discretize the ray by sampling points along that ray, and then have the model 
        target the true color from the training camera's image.
    </div>

    <div class="two-images">
        <div class="image-item">
            <img src="images/visualization.png" alt="Description of Image 1">
            <div class="description"> 
                Visualization of the training process. We sample a total of BATCH_SIZE number of rays, taken equally from 10 randomly sampled training cameras.
                The rays are discretized by sampling 3D points along the rays. During training, we add a small perturbation to the coordinates of the ray, so that
                they are not perfectly evenly spaced. The sampled points are shown as black dots along the black arrows. The square pyramids and images show camera 
                locations, as well as the image that the training camera sees.
            </div>
        </div>
    </div>
    <div class="two-images">
        <div class="image-item">
            <img src="images/architecture.png" alt="Description of Image 1">
            <div class="description"> 
                Neural Net architecture. We use positional encoding frequencies L=12 for the (x, y) pixels, and L=4 for the viewing direction. 
                Fully linear layers have hidden size = 384, and we concatenate the inputs at set points. The final output has a sigmoid activation for RGB, and ReLU
                for the density, which only has to be nonnegative.  <br>
                Hyperparams: BATCH_SIZE = 10,000. Learning rate 5e-4. Adam with default args. 1250 iterations for 5 epochs of 250 iterations. PSNR loss. Perturbation 
                with ray sampling of \( t = linspace(2.0, 6.0, N\_SAMPLES=64) + rand * (0.5 * t\_width) \), where t_width = (6.0 - 2.0) / N_SAMPLES.
            </div>
        </div>
    </div>
    <div class="one-image">
        <div class="image-item">
            <img src="images/training.png" alt="Description of Image 1">
            <div class="description"> 
                Visualization of training process over 5 epochs; camera perspective is taken from the validation set. This is a completely novel viewing angle, 
                not seen during training at all! 
                <br> 
                The model is interpolating between training image viewing directions, using an internal map of 3D space and density.
                Ground truth image is shown on the right.
            </div>
        </div>
    </div>
    <div class="two-images-center">
        <div class="image-item">
            <img src="images/nerf_trainingloss.png" alt="Description of Image 1">
            <div class="description"> 
                Training loss
            </div>
        </div>
        <div class="image-item">
            <img src="images/nerf_validationloss.png" alt="Description of Image 1">
            <div class="description"> 
                Validation loss taken after every epoch, averaged over 5 validation perspectives. 
                (Note that the axis is wrong— should be shifted over 250 iterations)
            </div>
        </div>
    </div>
    <div class="two-images">
        <div class="image-item">
            <img src="images/nerf_test.gif" alt="Description of Image 1">
            <div class="description"> 
                Spherical rendering of test cameras rotating around the lego truck! All perspectives are completely novel viewing directions.
            </div>
        </div>
    </div>

    <div class="two-images">
        <div class="image-item">
            <img src="images/nerf_test_blue.gif" alt="Description of Image 1">
            <div class="description"> 
                Bells and Whistles: The volume rendering equation can be viewed as traveling along a ray, and treating density as transparency. More dense and thus
                less transparent objects "donate" more of that 3D point's predicted color to the final RGB prediction for the viewing direction. By appending a 
                fake point for every ray that has high density and a selected color, we can color the background of the scene! For any ray that hits the object, 
                the fake background point's color will not be reached. However, for rays that "miss" the object (and that the model will output zero or low density for 
                the points along the ray), those pixels will take on the background color instead. Here, we use background color (0, 0, 255) in RGB space.
            </div>
        </div>
    </div>
</div>
</body>
</html>

